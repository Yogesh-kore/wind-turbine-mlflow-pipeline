{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wind Turbine Power Prediction - Model Training\n",
    "\n",
    "This notebook demonstrates the process of training and evaluating machine learning models for wind turbine power prediction using MLflow for experiment tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import necessary libraries and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))\n",
    "\n",
    "# Import project modules\n",
    "from src.data.load_data import load_data_from_csv, load_data_from_mongodb\n",
    "from src.data.preprocess import preprocess_data\n",
    "from src.models.train import train_model, train_models\n",
    "from src.models.evaluate import evaluate_model, evaluate_all_models, find_best_model\n",
    "from src.utils.mlflow_utils import setup_mlflow, log_experiment_to_mongodb, get_best_run, register_model\n",
    "from src.utils.visualization import plot_actual_vs_predicted, plot_residuals_distribution, plot_feature_importance\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Display all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure MLflow\n",
    "\n",
    "Set up MLflow tracking server and experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set MLflow tracking URI\n",
    "mlflow_uri = 'http://localhost:5000'\n",
    "experiment_name = 'wind_turbine_power_prediction'\n",
    "\n",
    "# Setup MLflow\n",
    "setup_mlflow(mlflow_uri, experiment_name)\n",
    "\n",
    "print(f'MLflow tracking URI: {mlflow.get_tracking_uri()}')\n",
    "print(f'MLflow experiment: {mlflow.get_experiment_by_name(experiment_name)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load the wind turbine data from MongoDB or CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB connection parameters\n",
    "mongodb_uri = 'mongodb://localhost:27017/'\n",
    "database_name = 'wind_turbine'\n",
    "collection_name = 'turbine_data'\n",
    "\n",
    "# Try to load from MongoDB first\n",
    "try:\n",
    "    df = load_data_from_mongodb(mongodb_uri, database_name, collection_name)\n",
    "    print(f'Data loaded from MongoDB with shape: {df.shape}')\n",
    "except Exception as e:\n",
    "    print(f'Error loading from MongoDB: {str(e)}')\n",
    "    print('Falling back to CSV...')\n",
    "    \n",
    "    # Load from CSV
    data_path = 'C:\\Users\\ADMIN\\Desktop\\Project\\wind-turbine-mlflow-pipeline\\windt_cleaned_iqr.csv'
    df = load_data_from_csv(data_path)
    print(f'Data loaded from CSV with shape: {df.shape}')
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n",
    "Preprocess the data for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify target variable (assuming it's Power_Output)\n",
    "target_col = 'Power_Output' if 'Power_Output' in df.columns else df.columns[-1]\n",
    "print(f'Target variable: {target_col}')\n",
    "\n",
    "# Identify features (all columns except target)\n",
    "feature_cols = [col for col in df.columns if col != target_col and col != '_id']\n",
    "print(f'Number of features: {len(feature_cols)}')\n",
    "print(f'Features: {feature_cols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "X_train, X_test, y_train, y_test, preprocessors = preprocess_data(\n",
    "    df, \n",
    "    target_col=target_col, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f'Training set shape: {X_train.shape}')\n",
    "print(f'Testing set shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models\n",
    "\n",
    "Train multiple regression models and track experiments with MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to train\n",
    "models_to_train = ['linear', 'random_forest', 'xgboost']\n",
    "\n",
    "# Train models\n",
    "trained_models = train_models(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    X_test, \n",
    "    y_test, \n",
    "    models=models_to_train,\n",
    "    log_to_mlflow=True\n",
    ")\n",
    "\n",
    "print(f'Trained models: {list(trained_models.keys())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Models\n",
    "\n",
    "Evaluate the trained models and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "evaluation_results = {}\n",
    "\n",
    "for model_name, model in trained_models.items():\n",
    "    metrics = evaluate_model(model, X_test, y_test)\n",
    "    evaluation_results[model_name] = metrics\n",
    "    \n",
    "    print(f'\\nModel: {model_name}')\n",
    "    print(f'RMSE: {metrics["rmse"]:.4f}')\n",
    "    print(f'MAE: {metrics["mae"]:.4f}')\n",
    "    print(f'R²: {metrics["r2"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with evaluation results for comparison\n",
    "results_df = pd.DataFrame(evaluation_results).T\n",
    "results_df = results_df.sort_values('rmse')  # Sort by RMSE (lower is better)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot RMSE\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.barplot(x=results_df.index, y='rmse', data=results_df)\n",
    "plt.title('RMSE Comparison')\n",
    "plt.ylabel('RMSE (lower is better)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Plot MAE\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.barplot(x=results_df.index, y='mae', data=results_df)\n",
    "plt.title('MAE Comparison')\n",
    "plt.ylabel('MAE (lower is better)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Plot R²\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.barplot(x=results_df.index, y='r2', data=results_df)\n",
    "plt.title('R² Comparison')\n",
    "plt.ylabel('R² (higher is better)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Best Model Performance\n",
    "\n",
    "Visualize the performance of the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model based on RMSE\n",
    "best_model_name = results_df.index[0]  # First row after sorting by RMSE\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "print(f'Best model: {best_model_name}')\n",
    "print(f'RMSE: {results_df.loc[best_model_name, "rmse"]:.4f}')\n",
    "print(f'MAE: {results_df.loc[best_model_name, "mae"]:.4f}')\n",
    "print(f'R²: {results_df.loc[best_model_name, "r2"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_actual_vs_predicted(y_test, y_pred, title=f'{best_model_name} - Actual vs Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residuals distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_residuals_distribution(y_test, y_pred, title=f'{best_model_name} - Residuals Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residuals vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "residuals = y_test - y_pred\n",
    "plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title(f'{best_model_name} - Residuals vs Predicted')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "Analyze feature importance for the best model (if applicable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model has feature importance attribute\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    # Get feature importance\n",
    "    feature_importance = best_model.feature_importances_\n",
    "    \n",
    "    # Create a DataFrame for feature importance\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Importance': feature_importance\n",
    "    })\n",
    "    \n",
    "    # Sort by importance\n",
    "    importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plot_feature_importance(importance_df, title=f'{best_model_name} - Feature Importance')\n",
    "    plt.show()\n",
    "    \n",
    "    # Display feature importance\n",
    "    importance_df\n",
    "elif best_model_name == 'linear' and hasattr(best_model, 'coef_'):\n",
    "    # For linear models, use coefficients as importance\n",
    "    coefficients = best_model.coef_\n",
    "    \n",
    "    # Create a DataFrame for coefficients\n",
    "    coef_df = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Coefficient': coefficients\n",
    "    })\n",
    "    \n",
    "    # Sort by absolute coefficient value\n",
    "    coef_df['Abs_Coefficient'] = np.abs(coef_df['Coefficient'])\n",
    "    coef_df = coef_df.sort_values('Abs_Coefficient', ascending=False)\n",
    "    \n",
    "    # Plot coefficients\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.barh(coef_df['Feature'], coef_df['Coefficient'])\n",
    "    plt.title(f'{best_model_name} - Feature Coefficients')\n",
    "    plt.xlabel('Coefficient Value')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display coefficients\n",
    "    coef_df[['Feature', 'Coefficient']]\n",
    "else:\n",
    "    print(f'Feature importance not available for {best_model_name} model.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Best Run from MLflow\n",
    "\n",
    "Retrieve the best run from MLflow based on RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best run from MLflow\n",
    "best_run = get_best_run(experiment_name, metric_name='rmse', mode='min')\n",
    "\n",
    "if best_run:\n",
    "    print(f'Best run ID: {best_run.info.run_id}')\n",
    "    print(f'Best run metrics: {best_run.data.metrics}')\n",
    "    print(f'Best run parameters: {best_run.data.params}')\n",
    "else:\n",
    "    print('No runs found in MLflow.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Best Model\n",
    "\n",
    "Register the best model in the MLflow Model Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the best model\n",
    "if best_run:\n",
    "    model_name = 'wind_turbine_power_model'\n",
    "    model_version = register_model(\n",
    "        best_run.info.run_id, \n",
    "        model_name, \n",
    "        stage='Production'\n",
    "    )\n",
    "    \n",
    "    print(f'Registered model: {model_name}')\n",
    "    print(f'Model version: {model_version}')\n",
    "    print(f'Model stage: Production')\n",
    "else:\n",
    "    print('No best run available to register.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Best Model Locally\n",
    "\n",
    "Save the best model to the local models directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model locally\n",
    "model_dir = '../models'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(model_dir, f'{best_model_name}_model.pkl')\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "    \n",
    "print(f'Best model saved to {model_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Summarize the model training and evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary\n",
    "print('Model Training Summary:')\n",
    "print(f'Number of samples: {len(X_train) + len(X_test)}')\n",
    "print(f'Number of features: {X_train.shape[1]}')\n",
    "print(f'Training set size: {len(X_train)}')\n",
    "print(f'Testing set size: {len(X_test)}')\n",
    "print('\\nModels trained:')\n",
    "for model_name in trained_models.keys():\n",
    "    print(f'- {model_name}')\n",
    "    \n",
    "print('\\nBest model:')\n",
    "print(f'- Model: {best_model_name}')\n",
    "print(f'- RMSE: {results_df.loc[best_model_name, "rmse"]:.4f}')\n",
    "print(f'- MAE: {results_df.loc[best_model_name, "mae"]:.4f}')\n",
    "print(f'- R²: {results_df.loc[best_model_name, "r2"]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Outline the next steps for model deployment and monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the model training and evaluation, here are the recommended next steps:\n",
    "\n",
    "1. **Model Deployment**:\n",
    "   - Deploy the best model as a REST API using Flask\n",
    "   - Containerize the application using Docker\n",
    "   - Set up CI/CD pipeline for automated deployment\n",
    "\n",
    "2. **Model Monitoring**:\n",
    "   - Set up monitoring for model performance\n",
    "   - Track prediction accuracy over time\n",
    "   - Detect data drift and model degradation\n",
    "\n",
    "3. **Model Improvement**:\n",
    "   - Collect feedback from users\n",
    "   - Incorporate new data for retraining\n",
    "   - Experiment with advanced models and feature engineering\n",
    "\n",
    "4. **Documentation and Knowledge Sharing**:\n",
    "   - Document the model architecture and performance\n",
    "   - Share insights with stakeholders\n",
    "   - Create dashboards for visualizing model performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}