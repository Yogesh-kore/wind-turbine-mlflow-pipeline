{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wind Turbine Data Exploratory Analysis\n",
    "\n",
    "This notebook performs exploratory data analysis on the wind turbine dataset to understand the data characteristics, distributions, and relationships between variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import necessary libraries and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))\n",
    "\n",
    "# Import project modules\n",
    "from src.data.load_data import load_data_from_csv, load_data_from_mongodb\n",
    "from src.utils.visualization import plot_correlation_matrix\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Display all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load the wind turbine data from CSV or MongoDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load from MongoDB first\n",
    "try:\n",
    "    df = load_data_from_mongodb()\n",
    "    print(f'Data loaded from MongoDB with shape: {df.shape}')\n",
    "except Exception as e:\n",
    "    print(f'Error loading from MongoDB: {str(e)}')\n",
    "    print('Falling back to CSV...')\n",
    "    \n",
    "    # Load from CSV
    data_path = 'C:\\Users\\ADMIN\\Desktop\\Project\\wind-turbine-mlflow-pipeline\\windt_cleaned_iqr.csv'
    df = load_data_from_csv(data_path)
    print(f'Data loaded from CSV with shape: {df.shape}')
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview\n",
    "\n",
    "Get a basic overview of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows\n",
    "print('\\nFirst 5 rows of the dataset:')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset information\n",
    "print('\\nDataset information:')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary statistics\n",
    "print('\\nSummary statistics:')\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print('\\nMissing values:')\n",
    "missing_values = df.isnull().sum()\n",
    "missing_values[missing_values > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Distributions\n",
    "\n",
    "Analyze the distributions of key variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric columns (excluding ID columns)\n",
    "numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Remove ID columns if present\n",
    "if '_id' in numeric_cols:\n",
    "    numeric_cols.remove('_id')\n",
    "\n",
    "print(f'Numeric columns: {numeric_cols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms for numeric columns\n",
    "plt.figure(figsize=(16, 12))\n",
    "for i, col in enumerate(numeric_cols[:9], 1):  # Limit to 9 columns for readability\n",
    "    plt.subplot(3, 3, i)\n",
    "    sns.histplot(df[col], kde=True)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot box plots for numeric columns to identify outliers\n",
    "plt.figure(figsize=(16, 12))\n",
    "for i, col in enumerate(numeric_cols[:9], 1):  # Limit to 9 columns for readability\n",
    "    plt.subplot(3, 3, i)\n",
    "    sns.boxplot(y=df[col])\n",
    "    plt.title(f'Box Plot of {col}')\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Variables\n",
    "\n",
    "Analyze categorical variables if present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select categorical columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f'Categorical columns: {categorical_cols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot count plots for categorical columns\n",
    "if categorical_cols:\n",
    "    plt.figure(figsize=(16, 4 * len(categorical_cols)))\n",
    "    for i, col in enumerate(categorical_cols, 1):\n",
    "        plt.subplot(len(categorical_cols), 1, i)\n",
    "        value_counts = df[col].value_counts().sort_values(ascending=False)\n",
    "        \n",
    "        # Limit to top 20 categories if there are too many\n",
    "        if len(value_counts) > 20:\n",
    "            value_counts = value_counts.head(20)\n",
    "            plt.title(f'Top 20 Categories in {col}')\n",
    "        else:\n",
    "            plt.title(f'Categories in {col}')\n",
    "            \n",
    "        sns.barplot(x=value_counts.index, y=value_counts.values)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No categorical columns found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis\n",
    "\n",
    "Analyze correlations between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix for numeric columns\n",
    "corr = df[numeric_cols].corr()\n",
    "\n",
    "# Plot correlation matrix\n",
    "plt.figure(figsize=(14, 12))\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0,\n",
    "            square=True, linewidths=.5, annot=True, fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the target variable (assuming it's Power_Output)\n",
    "target_col = 'Power_Output' if 'Power_Output' in df.columns else numeric_cols[-1]\n",
    "print(f'Target variable: {target_col}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations with the target variable\n",
    "target_corr = corr[target_col].sort_values(ascending=False)\n",
    "\n",
    "# Plot correlations with target\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=target_corr.index, y=target_corr.values)\n",
    "plt.title(f'Correlation with {target_col}')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Plots\n",
    "\n",
    "Create scatter plots to visualize relationships between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 5 correlated features with the target\n",
    "top_corr_features = target_corr[1:6].index.tolist()  # Exclude the target itself\n",
    "print(f'Top correlated features with {target_col}: {top_corr_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plots for top correlated features\n",
    "plt.figure(figsize=(16, 12))\n",
    "for i, feature in enumerate(top_corr_features, 1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    sns.scatterplot(x=df[feature], y=df[target_col], alpha=0.5)\n",
    "    plt.title(f'{feature} vs {target_col}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(target_col)\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pair Plot\n",
    "\n",
    "Create a pair plot to visualize relationships between multiple variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pair plot for the target and top correlated features\n",
    "pair_cols = top_corr_features + [target_col]\n",
    "sns.pairplot(df[pair_cols], height=2.5)\n",
    "plt.suptitle('Pair Plot of Top Correlated Features', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Analysis (if applicable)\n",
    "\n",
    "If the dataset contains time-related columns, perform time series analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any datetime columns\n",
    "datetime_cols = []\n",
    "for col in df.columns:\n",
    "    if 'date' in col.lower() or 'time' in col.lower():\n",
    "        try:\n",
    "            df[col] = pd.to_datetime(df[col])\n",
    "            datetime_cols.append(col)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "print(f'Datetime columns: {datetime_cols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If datetime columns exist, plot time series\n",
    "if datetime_cols:\n",
    "    time_col = datetime_cols[0]  # Use the first datetime column\n",
    "    \n",
    "    # Sort by time\n",
    "    df_sorted = df.sort_values(by=time_col)\n",
    "    \n",
    "    # Plot time series for the target variable\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.plot(df_sorted[time_col], df_sorted[target_col])\n",
    "    plt.title(f'{target_col} Over Time')\n",
    "    plt.xlabel(time_col)\n",
    "    plt.ylabel(target_col)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No datetime columns found for time series analysis.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Analysis\n",
    "\n",
    "Identify and analyze outliers in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to identify outliers using IQR method\n",
    "def identify_outliers(df, col):\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]\n",
    "    return outliers, lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze outliers in numeric columns\n",
    "for col in numeric_cols[:5]:  # Limit to first 5 columns for brevity\n",
    "    outliers, lower_bound, upper_bound = identify_outliers(df, col)\n",
    "    \n",
    "    print(f'\\nOutliers in {col}:')\n",
    "    print(f'Number of outliers: {len(outliers)}')\n",
    "    print(f'Percentage of outliers: {len(outliers) / len(df) * 100:.2f}%')\n",
    "    print(f'Lower bound: {lower_bound}')\n",
    "    print(f'Upper bound: {upper_bound}')\n",
    "    \n",
    "    if len(outliers) > 0:\n",
    "        print('Outlier statistics:')\n",
    "        print(outliers.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Ideas\n",
    "\n",
    "Based on the exploratory analysis, suggest potential feature engineering ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print feature engineering suggestions based on the data\n",
    "print('Feature Engineering Suggestions:')\n",
    "print('1. Handle outliers using IQR method or winsorization')\n",
    "print('2. Scale numeric features using StandardScaler or MinMaxScaler')\n",
    "print('3. Encode categorical variables using LabelEncoder or OneHotEncoder')\n",
    "\n",
    "# Add more specific suggestions based on the dataset\n",
    "if 'Wind_Speed' in df.columns and 'Direction' in df.columns:\n",
    "    print('4. Create wind vector components (Wind_Speed_X, Wind_Speed_Y) from Wind_Speed and Direction')\n",
    "\n",
    "if any('temp' in col.lower() for col in df.columns):\n",
    "    print('5. Create temperature-related features like temperature squared or temperature difference')\n",
    "\n",
    "if datetime_cols:\n",
    "    print('6. Extract time-based features like hour of day, day of week, month, season, etc.')\n",
    "    print('7. Create lag features for time series analysis')\n",
    "\n",
    "print('8. Consider polynomial features for highly correlated variables')\n",
    "print('9. Create interaction terms between highly correlated features')\n",
    "print('10. Apply dimensionality reduction techniques like PCA if there are many features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Summarize the key findings from the exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary statistics\n",
    "print('Dataset Summary:')\n",
    "print(f'Number of samples: {len(df)}')\n",
    "print(f'Number of features: {len(df.columns) - 1}')  # Excluding target\n",
    "print(f'Number of numeric features: {len(numeric_cols) - 1}')  # Excluding target\n",
    "print(f'Number of categorical features: {len(categorical_cols)}')\n",
    "print(f'Missing values: {df.isnull().sum().sum()}')\n",
    "\n",
    "# Print top correlated features with target\n",
    "print(f'\\nTop features correlated with {target_col}:')\n",
    "for feature, corr_value in target_corr[1:6].items():  # Exclude the target itself\n",
    "    print(f'{feature}: {corr_value:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Outline the next steps for the wind turbine power prediction project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the exploratory data analysis, here are the recommended next steps:\n",
    "\n",
    "1. **Data Preprocessing**:\n",
    "   - Handle missing values using appropriate imputation techniques\n",
    "   - Handle outliers using IQR method or winsorization\n",
    "   - Encode categorical variables\n",
    "   - Scale numeric features\n",
    "\n",
    "2. **Feature Engineering**:\n",
    "   - Create new features based on domain knowledge\n",
    "   - Generate polynomial features for highly correlated variables\n",
    "   - Create interaction terms between important features\n",
    "\n",
    "3. **Model Selection and Training**:\n",
    "   - Train multiple regression models (Linear Regression, Random Forest, XGBoost)\n",
    "   - Evaluate models using cross-validation\n",
    "   - Track experiments with MLflow\n",
    "\n",
    "4. **Model Evaluation**:\n",
    "   - Compare model performance using RMSE, MAE, and R²\n",
    "   - Analyze feature importance\n",
    "   - Examine residuals\n",
    "\n",
    "5. **Model Deployment**:\n",
    "   - Register the best model in MLflow\n",
    "   - Create a REST API for model serving\n",
    "   - Containerize the application using Docker\n",
    "\n",
    "6. **Monitoring and Maintenance**:\n",
    "   - Set up monitoring for model performance\n",
    "   - Implement a CI/CD pipeline for model updates\n",
    "   - Schedule regular retraining"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
